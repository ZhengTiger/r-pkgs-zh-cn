{"title":"Advanced testing techniques","markdown":{"headingText":"Advanced testing techniques","headingAttr":{"id":"sec-testing-advanced","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n```{r, echo = FALSE}\nsource(\"common.R\")\n```\n\n::: callout-important\nYour test files should not include these `library()` calls.\nWe also explicitly request testthat edition 3, but in a real package this will be declared in DESCRIPTION.\n\n```{r}\nlibrary(testthat)\nlocal_edition(3)\n```\n:::\n\n## Test fixtures\n\nWhen it's not practical to make your test entirely self-sufficient, prefer making the necessary object, logic, or conditions available in a structured, explicit way.\nThere's a pre-existing term for this in software engineering: a *test fixture*.\n\n> A test fixture is something used to consistently test some item, device, or piece of software.\n> --- Wikipedia\n\nThe main idea is that we need to make it as easy and obvious as possible to arrange the world into a state that is conducive for testing.\nWe describe several specific solutions to this problem:\n\n-   Put repeated code in a constructor-type helper function. Memoise it, if construction is demonstrably slow.\n-   If the repeated code has side effects, write a custom `local_*()` function to do what's needed and clean up afterwards.\n-   If the above approaches are too slow or awkward and the thing you need is fairly stable, save it as a static file and load it.\n\n```{=html}\n<!--\nI have not found a good example of memoising a test helper in the wild.\n\nHere's a clean little example of low-tech memoisation, taken from pillar, in\ncase I come back to this.\n\n# Only check if we have color support once per session\nnum_colors <- local({\n  num_colors <- NULL\n  function(forget = FALSE) {\n    if (is.null(num_colors) || forget) {\n      num_colors <<- cli::num_ansi_colors()\n    }\n    num_colors\n  }\n})\n-->\n```\n### Create `useful_thing`s with a helper function {#sec-testing-advanced-fixture-helper}\n\nIs it fiddly to create a `useful_thing`?\nDoes it take several lines of code, but not much time or memory?\nIn that case, write a helper function to create a `useful_thing` on-demand:\n\n```{r eval = FALSE}\nnew_useful_thing <- function() {\n  # your fiddly code to create a useful_thing goes here\n}\n```\n\nand call that helper in the affected tests:\n\n```{r eval = FALSE}\ntest_that(\"foofy() does this\", {\n  useful_thing1 <- new_useful_thing()\n  expect_equal(foofy(useful_thing1, x = \"this\"), EXPECTED_FOOFY_OUTPUT)\n})\n\ntest_that(\"foofy() does that\", {\n  useful_thing2 <- new_useful_thing()\n  expect_equal(foofy(useful_thing2, x = \"that\"), EXPECTED_FOOFY_OUTPUT)\n})\n```\n\nWhere should the `new_useful_thing()` helper be defined?\nThis comes back to what we outlined in @sec-tests-files-overview.\nTest helpers can be defined below `R/`, just like any other internal utility in your package.\nAnother popular location is in a test helper file, e.g. `tests/testthat/helper.R`.\nA key feature of both options is that the helpers are made available to you during interactive maintenance via `devtools::load_all()`.\n\nIf it's fiddly AND costly to create a `useful_thing`, your helper function could even use memoisation to avoid unnecessary re-computation.\nOnce you have a helper like `new_useful_thing()`, you often discover that it has uses beyond testing, e.g. behind-the-scenes in a vignette.\nSometimes you even realize you should just define it below `R/` and export and document it, so you can use it freely in documentation and tests.\n\n### Create (and destroy) a \"local\" `useful_thing`\n\nSo far, our example of a `useful_thing` was a regular R object, which is cleaned-up automatically at the end of each test.\nWhat if the creation of a `useful_thing` has a side effect on the local file system, on a remote resource, R session options, environment variables, or the like?\nThen your helper function should create a `useful_thing` **and clean up afterwards**.\nInstead of a simple `new_useful_thing()` constructor, you'll write a customized function in the style of withr's `local_*()` functions:\n\n```{r}\nlocal_useful_thing <- function(..., env = parent.frame()) {\n  # your fiddly code to create a useful_thing goes here\n  withr::defer(\n    # your fiddly code to clean up after a useful_thing goes here\n    envir = env\n  )\n}\n```\n\nUse it in your tests like this:\n\n```{r eval = FALSE}\ntest_that(\"foofy() does this\", {\n  useful_thing1 <- local_useful_thing()\n  expect_equal(foofy(useful_thing1, x = \"this\"), EXPECTED_FOOFY_OUTPUT)\n})\n\ntest_that(\"foofy() does that\", {\n  useful_thing2 <- local_useful_thing()\n  expect_equal(foofy(useful_thing2, x = \"that\"), EXPECTED_FOOFY_OUTPUT)\n})\n```\n\nWhere should the `local_useful_thing()` helper be defined?\nAll the advice given above for `new_useful_thing()` applies: define it below `R/` or in a test helper file.\n\nTo learn more about writing custom helpers like `local_useful_thing()`, see the [testthat vignette on test fixtures](https://testthat.r-lib.org/articles/test-fixtures.html).\n\n### Store a concrete `useful_thing` persistently {#sec-testing-advanced-concrete-fixture}\n\nIf a `useful_thing` is costly to create, in terms of time or memory, maybe you don't actually need to re-create it for each test run.\nYou could make the `useful_thing` once, store it as a static test fixture, and load it in the tests that need it.\nHere's a sketch of how this could look:\n\n```{r eval = FALSE}\ntest_that(\"foofy() does this\", {\n  useful_thing1 <- readRDS(test_path(\"fixtures\", \"useful_thing1.rds\"))\n  expect_equal(foofy(useful_thing1, x = \"this\"), EXPECTED_FOOFY_OUTPUT)\n})\n\ntest_that(\"foofy() does that\", {\n  useful_thing2 <- readRDS(test_path(\"fixtures\", \"useful_thing2.rds\"))\n  expect_equal(foofy(useful_thing2, x = \"that\"), EXPECTED_FOOFY_OUTPUT)\n})\n```\n\nNow we can revisit a file listing from earlier, which addressed exactly this scenario:\n\n```         \n.\nâ”œâ”€â”€ ...\nâ””â”€â”€ tests\n    â”œâ”€â”€ testthat\n    â”‚   â”œâ”€â”€ fixtures\n    â”‚   â”‚   â”œâ”€â”€ make-useful-things.R\n    â”‚   â”‚   â”œâ”€â”€ useful_thing1.rds\n    â”‚   â”‚   â””â”€â”€ useful_thing2.rds\n    â”‚   â”œâ”€â”€ helper.R\n    â”‚   â”œâ”€â”€ setup.R\n    â”‚   â””â”€â”€ (all the test files)\n    â””â”€â”€ testthat.R\n```\n\nThis shows static test files stored in `tests/testthat/fixtures/`, but also notice the companion R script, `make-useful-things.R`.\nFrom data analysis, we all know there is no such thing as a script that is run only once.\nRefinement and iteration is inevitable.\nThis also holds true for test objects like `useful_thing1.rds`.\nWe highly recommend saving the R code used to create your test objects, so that they can be re-created as needed.\n\n## Building your own testing tools\n\nLet's return to the topic of duplication in your test code.\nWe've encouraged you to have a higher tolerance for repetition in test code, in the name of making your tests obvious.\nBut there's still a limit to how much repetition to tolerate.\nWe've covered techniques such as loading static objects with `test_path()`, writing a constructor like `new_useful_thing()`, or implementing a test fixture like `local_useful_thing()`.\nThere are even more types of test helpers that can be useful in certain situations.\n\n### Helper defined inside a test\n\nConsider this test for the `str_trunc()` function in stringr:\n\n```{r eval = FALSE}\n# from stringr (hypothetically)\ntest_that(\"truncations work for all sides\", {\n  expect_equal(\n    str_trunc(\"This string is moderately long\", width = 20, side = \"right\"),\n    \"This string is mo...\"\n  )\n  expect_equal(\n    str_trunc(\"This string is moderately long\", width = 20, side = \"left\"),\n    \"...s moderately long\"\n  )\n  expect_equal(\n    str_trunc(\"This string is moderately long\", width = 20, side = \"center\"),\n    \"This stri...ely long\"\n  )\n})\n```\n\nThere's a lot of repetition here, which increases the chance of copy / paste errors and generally makes your eyes glaze over.\nSometimes it's nice to create a hyper-local helper, *inside the test*.\nHere's how the test actually looks in stringr\n\n```{r eval = FALSE}\n# from stringr (actually)\ntest_that(\"truncations work for all sides\", {\n\n  trunc <- function(direction) str_trunc(\n    \"This string is moderately long\",\n    direction,\n    width = 20\n  )\n\n  expect_equal(trunc(\"right\"),   \"This string is mo...\")\n  expect_equal(trunc(\"left\"),    \"...s moderately long\")\n  expect_equal(trunc(\"center\"),  \"This stri...ely long\")\n})\n```\n\nA hyper-local helper like `trunc()` is particularly useful when it allows you to fit all the important business for each expectation on one line.\nThen your expectations can be read almost like a table of actual vs. expected, for a set of related use cases.\nAbove, it's very easy to watch the result change as we truncate the input from the right, left, and in the center.\n\nNote that this technique should be used in extreme moderation.\nA helper like `trunc()` is yet another place where you can introduce a bug, so it's best to keep such helpers extremely short and simple.\n\n### Custom expectations\n\nIf a more complicated helper feels necessary, it's a good time to reflect on why that is.\nIf it's fussy to get into position to *test* a function, that could be a sign that it's also fussy to *use* that function.\nDo you need to refactor it?\nIf the function seems sound, then you probably need to use a more formal helper, defined outside of any individual test, as described earlier.\n\nOne specific type of helper you might want to create is a custom expectation.\nHere are two very simple ones from usethis:\n\n```{r}\nexpect_usethis_error <- function(...) {\n  expect_error(..., class = \"usethis_error\")\n}\n\nexpect_proj_file <- function(...) {\n  expect_true(file_exists(proj_path(...)))\n}\n```\n\n`expect_usethis_error()` checks that an error has the `\"usethis_error\"` class.\n`expect_proj_file()` is a simple wrapper around `file_exists()` that searches for the file in the current project.\nThese are very simple functions, but the sheer amount of repetition and the expressiveness of their names makes them feel justified.\n\nIt is somewhat involved to make a proper custom expectation, i.e. one that behaves like the expectations built into testthat.\nWe refer you to the [Custom expectations](https://testthat.r-lib.org/articles/custom-expectation.html) vignette if you wish to learn more about that.\n\nFinally, it can be handy to know that testthat makes specific information available when it's running:\n\n-   The environment variable `TESTTHAT` is set to `\"true\"`.\n    `testthat::is_testing()` is a shortcut:\n\n    ```{r, eval = FALSE}\n    is_testing <- function() {\n      Sys.getenv(\"TESTTHAT\")\n    }\n    ```\n\n-   The package-under-test is available as the environment variable `TESTTHAT_PKG` and `testthat::testing_package()` is a shortcut:\n\n    ```{r, eval = FALSE}\n    testing_package <- function() {\n      Sys.getenv(\"TESTTHAT_PKG\")\n    }\n    ```\n\nIn some situations, you may want to exploit this information without taking a run-time dependency on testthat.\nIn that case, just inline the source of these functions directly into your package.\n\n## When testing gets hard\n\nDespite all the techniques we've covered so far, there remain situations where it still feels very difficult to write tests.\nIn this section, we review more ways to deal with challenging situations:\n\n-   Skipping a test in certain situations\n-   Mocking an external service\n-   Dealing with secrets\n\n### Skipping a test {#tests-skipping}\n\nSometimes it's impossible to perform a test - you may not have an internet connection or you may not have access to the necessary credentials.\nUnfortunately, another likely reason follows from this simple rule: the more platforms you use to test your code, the more likely it is that you won't be able to run all of your tests, all of the time.\nIn short, there are times when, instead of getting a failure, you just want to skip a test.\n\n#### `testthat::skip()`\n\nHere we use `testthat::skip()` to write a hypothetical custom skipper, `skip_if_no_api()`:\n\n```{r, eval = FALSE}\nskip_if_no_api() <- function() {\n  if (api_unavailable()) {\n    skip(\"API not available\")\n  }\n}\n\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if_no_api()\n  ...\n})\n```\n\n`skip_if_no_api()` is a yet another example of a test helper and the advice already given about where to define it applies here too.\n\n`skip()`s and the associated reasons are reported inline as tests are executed and are also indicated clearly in the summary:\n\n```{r, eval = FALSE}\ndevtools::test()\n#> â„¹ Loading abcde\n#> â„¹ Testing abcde\n#> âœ” | F W S  OK | Context\n#> âœ” |         2 | blarg\n#> âœ” |     1   2 | foofy\n#> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> Skip (test-foofy.R:6:3): foo api returns bar when given baz\n#> Reason: API not available\n#> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> âœ” |         0 | yo                                                              \n#> â•â• Results â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#> â”€â”€ Skipped tests  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> â€¢ API not available (1)\n#> \n#> [ FAIL 0 | WARN 0 | SKIP 1 | PASS 4 ]\n#> \n#> ðŸ¥³\n```\n\nSomething like `skip_if_no_api()` is likely to appear many times in your test suite.\nThis is another occasion where it is tempting to DRY things out, by hoisting the `skip()` to the top-level of the file.\nHowever, we still lean towards calling `skip_if_no_api()` in each test where it's needed.\n\n```{r eval = FALSE}\n# we prefer this:\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if_no_api()\n  ...\n})\n\ntest_that(\"foo api returns an errors when given qux\", {\n  skip_if_no_api()\n  ...\n})\n\n# over this:\nskip_if_no_api()\n\ntest_that(\"foo api returns bar when given baz\", {...})\n\ntest_that(\"foo api returns an errors when given qux\", {...})\n```\n\nWithin the realm of top-level code in test files, having a `skip()` at the very beginning of a test file is one of the more benign situations.\nBut once a test file does not fit entirely on your screen, it creates an implicit yet easy-to-miss connection between the `skip()` and individual tests.\n\n#### Built-in `skip()` functions\n\nSimilar to testthat's built-in expectations, there is a family of `skip()` functions that anticipate some common situations.\nThese functions often relieve you of the need to write a custom skipper.\nHere are some examples of the most useful `skip()` functions:\n\n```{r eval = FALSE}\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if(api_unavailable(), \"API not available\")\n  ...\n})\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if_not(api_available(), \"API not available\")\n  ...\n})\n\nskip_if_not_installed(\"sp\")\nskip_if_not_installed(\"stringi\", \"1.2.2\")\n\nskip_if_offline()\nskip_on_cran()\nskip_on_os(\"windows\")\n```\n\n#### Dangers of skipping\n\nOne challenge with skips is that they are currently completely invisible in CI --- if you automatically skip too many tests, it's easy to fool yourself that all your tests are passing when in fact they're just being skipped!\nIn an ideal world, your CI/CD would make it easy to see how many tests are being skipped and how that changes over time.\n\nIt's a good practice to regularly dig into the `R CMD check` results, especially on CI, and make sure the skips are as you expect.\nBut this tends to be something you have to learn through experience.\n\n### Mocking\n\nThe practice known as mocking is when we replace something that's complicated or unreliable or out of our control with something simpler, that's fully within our control.\nUsually we are mocking an external service, such as a REST API, or a function that reports something about session state, such as whether the session is interactive.\n\nThe classic application of mocking is in the context of a package that wraps an external API.\nIn order to test your functions, technically you need to make a live call to that API to get a response, which you then process.\nBut what if that API requires authentication or what if it's somewhat flaky and has occasional downtime?\nIt can be more productive to just *pretend* to call the API but, instead, to test the code under your control by processing a pre-recorded response from the actual API.\n\nOur main advice about mocking is to avoid it if you can.\nThis is not an indictment of mocking, but just a realistic assessment that mocking introduces new complexity that is not always justified by the payoffs.\n\nSince most R packages do not need full-fledged mocking, we do not cover it here.\nInstead we'll point you to the packages that represent the state-of-the-art for mocking in R today:\n\n-   mockery: <https://github.com/r-lib/mockery>\n-   mockr: <https://krlmlr.github.io/mockr/>\n-   httptest: <https://enpiar.com/r/httptest/>\n-   httptest2: <https://enpiar.com/httptest2/>\n-   webfakes: <https://webfakes.r-lib.org>\n\nNote also that, at the time of writing, it seems likely that the testthat package will re-introduce some mocking capabilities (after previously getting out of the mocking business once already).\nVersion v3.1.7 has two new experimental functions, `testthat::with_mocked_bindings()` and `testthat::local_mocked_bindings()`.\n\n### Secrets\n\nAnother common challenge for packages that wrap an external service is the need to manage credentials.\nSpecifically, it is likely that you will need to provide a set of test credentials to fully test your package.\n\nOur main advice here is to design your package so that large parts of it can be tested without live, authenticated access to the external service.\n\nOf course, you will still want to be able to test your package against the actual service that it wraps, in environments that support secure environment variables.\nSince this is also a very specialized topic, we won't go into more detail here.\nInstead we refer you to the [Wrapping APIs](https://httr2.r-lib.org/articles/wrapping-apis.html#secret-management) vignette in the httr2 package, which offers substantial support for secret management.\n\n## Special considerations for CRAN packages\n\nCRAN runs `R CMD check` on all contributed packages, both upon submission and on a regular basis after acceptance.\nThis check includes, but is not limited to, your testthat tests.\nWe discuss the general challenge of preparing your package to face all of CRAN's check \"flavors\" in @sec-cran-flavors-services.\nHere we focus on CRAN-specific considerations for your test suite.\n\nWhen a package runs afoul of the CRAN Repository Policy (<https://cran.r-project.org/web/packages/policies.html>), the test suite is very often the culprit (although not always).\nIf your package is destined for CRAN, this should influence how you write your tests and how (or whether) they will be run on CRAN.\n\n### Skip a test {#sec-testing-advanced-skip-on-cran}\n\nIf a specific test simply isn't appropriate to be run by CRAN, include `skip_on_cran()` at the very start.\n\n```{r}\n#| eval: false\ntest_that(\"some long-running thing works\", {\n  skip_on_cran()\n  # test code that can potentially take \"a while\" to run  \n})\n```\n\nUnder the hood, `skip_on_cran()` consults the `NOT_CRAN` environment variable.\nSuch a test will only run when `NOT_CRAN` has been explicitly defined as `\"true\"`.\nThis variable is set by devtools and testthat, allowing those tests to run in environments where you expect success (and where you can tolerate and troubleshoot occasional failure).\n\nIn particular, the GitHub Actions workflows that we recommend in @sec-sw-dev-practices-gha **will** run tests with `NOT_CRAN = \"true\"`.\nFor certain types of functionality, there is no practical way to test it on CRAN and your own checks, on GitHub Actions or an equivalent continuous integration service, are your best method of quality assurance.\n\nThere are even rare cases where it makes sense to maintain tests outside of your package altogether.\nThe tidymodels team uses this strategy for integration-type tests of their whole ecosystem that would be impossible to host inside an individual CRAN package.\n\n### Speed\n\nYour tests need to run relatively quickly - ideally, less than a minute, in total.\nUse `skip_on_cran()` in a test that is unavoidably long-running.\n\n### Reproducibility\n\nBe careful about testing things that are likely to be variable on CRAN machines.\nIt's risky to test how long something takes (because CRAN machines are often heavily loaded) or to test parallel code (because CRAN runs multiple package tests in parallel, multiple cores will not always be available).\nNumerical precision can also vary across platforms, so use `expect_equal()` unless you have a specific reason for using `expect_identical()`.\n\n### Flaky tests {#sec-testing-advanced-flaky-tests}\n\nDue to the scale at which CRAN checks packages, there is basically no latitude for a test that's \"just flaky\", i.e. sometimes fails for incidental reasons.\nCRAN does not process your package's test results the way you do, where you can inspect each failure and exercise some human judgment about how concerning it is.\n\nIt's probably a good idea to eliminate flaky tests, just for your own sake!\nBut if you have valuable, well-written tests that are prone to occasional nuisance failure, definitely put `skip_on_cran()` at the start.\n\nThe classic example is any test that accesses a website or web API.\nGiven that any web resource in the world will experience occasional downtime, it's best to not let such tests run on CRAN.\nThe CRAN Repository Policy says:\n\n> Packages which use Internet resources should fail gracefully with an informative message if the resource is not available or has changed (and not give a check warning nor error).\n\nOften making such a failure \"graceful\" would run counter to the behaviour you actually want in practice, i.e. you would want your user to get an error if their request fails.\nThis is why it is usually more practical to test such functionality elsewhere.\n\nRecall that snapshot tests (@sec-testing-basics), by default, are also skipped on CRAN.\nYou typically use such tests to monitor, e.g., how various informational messages look.\nSlight changes in message formatting are something you want to be alerted to, but do not indicate a major defect in your package.\nThis is the motivation for the default `skip_on_cran()` behaviour of snapshot tests.\n\nFinally, flaky tests cause problems for the maintainers of your dependencies.\nWhen the packages you depend on are updated, CRAN runs `R CMD check` on all reverse dependencies, including your package.\nIf your package has flaky tests, your package can be the reason another package does not clear CRAN's incoming checks and can delay its release.\n\n### Process and file system hygiene\n\nIn @sec-tests-files-where-write, we urged you to only write into the session temp directory and to clean up after yourself.\nThis practice makes your test suite much more maintainable and predictable.\nFor packages that are (or aspire to be) on CRAN, this is absolutely required per the CRAN repository policy:\n\n> Packages should not write in the user's home filespace (including clipboards), nor anywhere else on the file system apart from the R session's temporary directory (or during installation in the location pointed to by TMPDIR: and such usage should be cleaned up)....\n> Limited exceptions may be allowed in interactive sessions if the package obtains confirmation from the user.\n\nSimilarly, you should make an effort to be hygienic with respect to any processes you launch:\n\n> Packages should not start external software (such as PDF viewers or browsers) during examples or tests unless that specific instance of the software is explicitly closed afterwards.\n\nAccessing the clipboard is the perfect storm that potentially runs afoul of both of these guidelines, as the clipboard is considered part of the user's home filespace and, on Linux, can launch an external process (e.g. xsel or xclip).\nTherefore it is best to turn off any clipboard functionality in your tests (and to ensure that, during authentic usage, your user is clearly opting-in to that).\n\n```{=html}\n<!--\nCreating and maintaining a healthy test suite takes real effort. As a codebase grows, so too will the test suite. It will begin to face challenges like instability and slowness. A failure to address these problems will cripple a test suite. Keep in mind that tests derive their value from the trust engineers place in them. If testing becomes a productivity sink, constantly inducing toil and uncertainty, engineers will lose trust and begin to find workarounds. A bad test suite can be worse than no test suite at all.\n\nRemember that tests are often revisited only when something breaks. When you are called to fix a broken test that you have never seen before, you will be thankful someone took the time to make it easy to understand. Code is read far more than it is written, so make sure you write the test youâ€™d like to read!\n\nhttps://abseil.io/resources/swe-book/html/ch11.html\n\nBecause they make up such a big part of engineersâ€™ lives, Google puts a lot of focus on test maintainability. Maintainable tests  are ones that \"just work\": after writing them, engineers donâ€™t need to think about them again until they fail, and those failures indicate real bugs with clear causes. The bulk of this chapter focuses on exploring the idea of maintainability and techniques for achieving it.\n\nhttps://abseil.io/resources/swe-book/html/ch12.html\n-->\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["plausible.html"],"output-file":"testing-advanced.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","bibliography":["book.bib"],"editor":"source","mainfont":"Microsoft Yahei","theme":["cosmo","custom.scss"]},"extensions":{"book":{"multiFile":true}}}}}