{"title":"Designing your test suite","markdown":{"headingText":"Designing your test suite","headingAttr":{"id":"sec-testing-design","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n```{r, echo = FALSE}\nsource(\"common.R\")\n```\n\n::: callout-important\nYour test files should not include these `library()` calls.\nWe also explicitly request testthat edition 3, but in a real package this will be declared in DESCRIPTION.\n\n```{r}\nlibrary(testthat)\nlocal_edition(3)\n```\n:::\n\n## What to test\n\n> Whenever you are tempted to type something into a print statement or a debugger expression, write it as a test instead.\n> --- Martin Fowler\n\nThere is a fine balance to writing tests.\nEach test that you write makes your code less likely to change inadvertently; but it also can make it harder to change your code on purpose.\nIt's hard to give good general advice about writing tests, but you might find these points helpful:\n\n-   Focus on testing the external interface to your functions - if you test the internal interface, then it's harder to change the implementation in the future because as well as modifying the code, you'll also need to update all the tests.\n\n-   Strive to test each behaviour in one and only one test.\n    Then if that behaviour later changes you only need to update a single test.\n\n-   Avoid testing simple code that you're confident will work.\n    Instead focus your time on code that you're not sure about, is fragile, or has complicated interdependencies.\n    That said, we often find we make the most mistakes when we falsely assume that the problem is simple and doesn't need any tests.\n\n-   Always write a test when you discover a bug.\n    You may find it helpful to adopt the test-first philosophy.\n    There you always start by writing the tests, and then write the code that makes them pass.\n    This reflects an important problem solving strategy: start by establishing your success criteria, how you know if you've solved the problem.\n\n### Test coverage {#sec-testing-design-coverage}\n\nAnother concrete way to direct your test writing efforts is to examine your test coverage.\nThe covr package (<https://covr.r-lib.org>) can be used to determine which lines of your package's source code are (or are not!) executed when the test suite is run.\nThis is most often presented as a percentage.\nGenerally speaking, the higher the better.\n\nIn some technical sense, 100% test coverage is the goal, however, this is rarely achieved in practice and that's often OK.\nGoing from 90% or 99% coverage to 100% is not always the best use of your development time and energy.\nIn many cases, that last 10% or 1% often requires some awkward gymnastics to cover.\nSometimes this forces you to introduce mocking or some other new complexity.\nDon't sacrifice the maintainability of your test suite in the name of covering some weird edge case that hasn't yet proven to be a problem.\nAlso remember that not every line of code or every function is equally likely to harbor bugs.\nFocus your testing energy on code that is tricky, based on your expert opinion and any empirical evidence you've accumulated about bug hot spots.\n\nWe use covr regularly, in two different ways:\n\n-   Local, interactive use. We mostly use `devtools::test_coverage_active_file()` and `devtools::test_coverage()`, for exploring the coverage of an individual file or the whole package, respectively.\n-   Automatic, remote use via GitHub Actions (GHA). We cover continuous integration and GHA more thoroughly in @sec-sw-dev-practices, but we will at least mention here that `usethis::use_github_action(\"test-coverage\")` configures a GHA workflow that constantly monitors your test coverage. Test coverage can be an especially helpful metric when evaluating a pull request (either your own or from an external contributor). A proposed change that is well-covered by tests is less risky to merge.\n\n## High-level principles for testing {#sec-testing-design-principles}\n\nIn later sections, we offer concrete strategies for how to handle common testing dilemmas in R.\nHere we lay out the high-level principles that underpin these recommendations:\n\n-   A test should ideally be self-sufficient and self-contained.\n-   The interactive workflow is important, because you will mostly interact with your tests when they are failing.\n-   It's more important that test code be obvious than, e.g., as DRY as possible.\n-   However, the interactive workflow shouldn't \"leak\" into and undermine the test suite.\n\nWriting good tests for a code base often feels more challenging than writing the code in the first place.\nThis can come as a bit of a shock when you're new to package development and you might be concerned that you're doing it wrong.\nDon't worry, you're not!\nTesting presents many unique challenges and maneuvers, which tend to get much less air time in programming communities than strategies for writing the \"main code\", i.e. the stuff below `R/`.\nAs a result, it requires more deliberate effort to develop your skills and taste around testing.\n\nMany of the packages maintained by our team violate some of the advice you'll find here.\nThere are (at least) two reasons for that:\n\n-   testthat has been evolving for more than twelve years and this chapter reflects the cumulative lessons learned from that experience. The tests in many packages have been in place for a long time and reflect typical practices from different eras and different maintainers.\n-   These aren't hard and fast rules, but are, rather, guidelines. There will always be specific situations where it makes sense to bend the rule.\n\nThis chapter can't address all possible testing situations, but hopefully these guidelines will help your future decision-making.\n\n### Self-sufficient tests\n\n> All tests should strive to be hermetic: a test should contain all of the information necessary to set up, execute, and tear down its environment.\n> Tests should assume as little as possible about the outside environment ....\n>\n> From the book Software Engineering at Google, [Chapter 11](https://abseil.io/resources/swe-book/html/ch11.html)\n\nRecall this advice found in @sec-code-r-landscape, which covers your package's \"main code\", i.e. everything below `R/`:\n\n> The `.R` files below `R/` should consist almost entirely of function definitions.\n> Any other top-level code is suspicious and should be carefully reviewed for possible conversion into a function.\n\nWe have analogous advice for your test files:\n\n> The `test-*.R` files below `tests/testthat/` should consist almost entirely of calls to `test_that()`.\n> Any other top-level code is suspicious and should be carefully considered for relocation into calls to `test_that()` or to other files that get special treatment inside an R package or from testthat.\n\nEliminating (or at least minimizing) top-level code outside of `test_that()` will have the beneficial effect of making your tests more hermetic.\nThis is basically the testing analogue of the general programming advice that it's wise to avoid unstructured sharing of state.\n\nLogic at the top-level of a test file has an awkward scope: Objects or functions defined here have what you might call \"test file scope\", if the definitions appear before the first call to `test_that()`.\nIf top-level code is interleaved between `test_that()` calls, you can even create \"partial test file scope\".\n\nWhile writing tests, it can feel convenient to rely on these file-scoped objects, especially early in the life of a test suite, e.g. when each test file fits on one screen.\nBut we find that implicitly relying on objects in a test's parent environment tends to make a test suite harder to understand and maintain over time.\n\nConsider a test file with top-level code sprinkled around it, outside of `test_that()`:\n\n```{r, eval = FALSE}\ndat <- data.frame(x = c(\"a\", \"b\", \"c\"), y = c(1, 2, 3))\n\nskip_if(today_is_a_monday())\n\ntest_that(\"foofy() does this\", {\n  expect_equal(foofy(dat), ...)\n})\n\ndat2 <- data.frame(x = c(\"x\", \"y\", \"z\"), y = c(4, 5, 6))\n\nskip_on_os(\"windows\")\n\ntest_that(\"foofy2() does that\", {\n  expect_snapshot(foofy2(dat, dat2))\n})\n```\n\nWe recommend relocating file-scoped logic to either a narrower scope or to a broader scope.\nHere's what it would look like to use a narrow scope, i.e. to inline everything inside `test_that()` calls:\n\n```{r, eval = FALSE}\ntest_that(\"foofy() does this\", {\n  skip_if(today_is_a_monday())\n  \n  dat <- data.frame(x = c(\"a\", \"b\", \"c\"), y = c(1, 2, 3))\n  \n  expect_equal(foofy(dat), ...)\n})\n\ntest_that(\"foofy() does that\", {\n  skip_if(today_is_a_monday())\n  skip_on_os(\"windows\")\n  \n  dat <- data.frame(x = c(\"a\", \"b\", \"c\"), y = c(1, 2, 3))\n  dat2 <- data.frame(x = c(\"x\", \"y\", \"z\"), y = c(4, 5, 6))\n  \n  expect_snapshot(foofy(dat, dat2))\n})\n```\n\nBelow we will discuss techniques for moving file-scoped logic to a broader scope.\n\n### Self-contained tests {#sec-testing-design-self-contained}\n\nEach `test_that()` test has its own execution environment, which makes it somewhat self-contained.\nFor example, an R object you create inside a test does not exist after the test exits:\n\n```{r}\nexists(\"thingy\")\n\ntest_that(\"thingy exists\", {\n  thingy <- \"thingy\"\n  expect_true(exists(thingy))\n})\n\nexists(\"thingy\")\n```\n\nThe `thingy` object lives and dies entirely within the confines of `test_that()`.\nHowever, testthat doesn't know how to cleanup after actions that affect other aspects of the R landscape:\n\n-   The filesystem: creating and deleting files, changing the working directory, etc.\n-   The search path: `library()`, `attach()`.\n-   Global options, like `options()` and `par()`, and environment variables.\n\nWatch how calls like `library()`, `options()`, and `Sys.setenv()` have a persistent effect *after* a test, even when they are executed inside `test_that()`:\n\n```{r}\ngrep(\"jsonlite\", search(), value = TRUE)\ngetOption(\"opt_whatever\")\nSys.getenv(\"envvar_whatever\")\n\ntest_that(\"landscape changes leak outside the test\", {\n  library(jsonlite)\n  options(opt_whatever = \"whatever\")\n  Sys.setenv(envvar_whatever = \"whatever\")\n  \n  expect_match(search(), \"jsonlite\", all = FALSE)\n  expect_equal(getOption(\"opt_whatever\"), \"whatever\")\n  expect_equal(Sys.getenv(\"envvar_whatever\"), \"whatever\")\n})\n\ngrep(\"jsonlite\", search(), value = TRUE)\ngetOption(\"opt_whatever\")\nSys.getenv(\"envvar_whatever\")\n```\n\nThese changes to the landscape even persist beyond the current test file, i.e. they carry over into all subsequent test files.\n\nIf it's easy to avoid making such changes in your test code, that is the best strategy!\nBut if it's unavoidable, then you have to make sure that you clean up after yourself.\nThis mindset is very similar to one we advocated for in @sec-code-r-landscape, when discussing how to design well-mannered functions.\n\n```{r, include = FALSE}\ndetach(\"package:jsonlite\")\noptions(opt_whatever = NULL)\nSys.unsetenv(\"envvar_whatever\")\n```\n\nWe like to use the withr package (<https://withr.r-lib.org>) to make temporary changes in global state, because it automatically captures the initial state and arranges the eventual restoration.\nYou've already seen an example of its usage, when we explored snapshot tests:\n\n```{r eval = FALSE}\ntest_that(\"side-by-side diffs work\", {\n  withr::local_options(width = 20) # <-- (°_°) look here!\n  expect_snapshot(\n    waldo::compare(c(\"X\", letters), c(letters, \"X\"))\n  )\n})\n```\n\nThis test requires the display width to be set at 20 columns, which is considerably less than the default width.\n`withr::local_options(width = 20)` sets the `width` option to 20 and, at the end of the test, restores the option to its original value.\nwithr is also pleasant to use during interactive development: deferred actions are still captured on the global environment and can be executed explicitly via `withr::deferred_run()` or implicitly by restarting R.\n\nWe recommend including withr in `Suggests`, if you're only going to use it in your tests, or in `Imports`, if you also use it below `R/`.\nCall withr functions as we do above, e.g. like `withr::local_whatever()`, in either case.\nSee @sec-dependencies-imports-vs-depends and @sec-dependencies-in-suggests-in-tests for more.\n\n::: callout-tip\nThe easiest way to add a package to DESCRIPTION is with, e.g., `usethis::use_package(\"withr\", type = \"Suggests\")`.\nFor tidyverse packages, withr is considered a \"free dependency\", i.e. the tidyverse uses withr so extensively that we don't hesitate to use it whenever it would be useful.\n:::\n\nwithr has a large set of pre-implemented `local_*()` / `with_*()` functions that should handle most of your testing needs, so check there before you write your own.\nIf nothing exists that meets your need, `withr::defer()` is the general way to schedule some action at the end of a test.[^testing-design-1]\n\n[^testing-design-1]: Base R's `on.exit()` is another alternative, but it requires more from you.\n    You need to capture the original state and write the restoration code yourself.\n    Also remember to do `on.exit(..., add = TRUE)` if there's *any* chance a second `on.exit()` call could be added in the test.\n    You probably also want to default to `after = FALSE`.\n\nHere's how we would fix the problems in the previous example using withr: *Behind the scenes, we reversed the landscape changes, so we can try this again.*\n\n```{r}\ngrep(\"jsonlite\", search(), value = TRUE)\ngetOption(\"opt_whatever\")\nSys.getenv(\"envvar_whatever\")\n\ntest_that(\"withr makes landscape changes local to a test\", {\n  withr::local_package(\"jsonlite\")\n  withr::local_options(opt_whatever = \"whatever\")\n  withr::local_envvar(envvar_whatever = \"whatever\")\n  \n  expect_match(search(), \"jsonlite\", all = FALSE)\n  expect_equal(getOption(\"opt_whatever\"), \"whatever\")\n  expect_equal(Sys.getenv(\"envvar_whatever\"), \"whatever\")\n})\n\ngrep(\"jsonlite\", search(), value = TRUE)\ngetOption(\"opt_whatever\")\nSys.getenv(\"envvar_whatever\")\n```\n\ntestthat leans heavily on withr to make test execution environments as reproducible and self-contained as possible.\nIn testthat 3e, `testthat::local_reproducible_output()` is implicitly part of each `test_that()` test.\n\n```{r, eval = FALSE}\ntest_that(\"something specific happens\", {\n  local_reproducible_output() # <-- this happens implicitly\n  \n  # your test code, which might be sensitive to ambient conditions, such as\n  # display width or the number of supported colors\n})\n```\n\n`local_reproducible_output()` temporarily sets various options and environment variables to values favorable for testing, e.g. it suppresses colored output, turns off fancy quotes, sets the console width, and sets `LC_COLLATE = \"C\"`.\nUsually, you can just passively enjoy the benefits of `local_reproducible_output()`.\nBut you may want to call it explicitly when replicating test results interactively or if you want to override the default settings in a specific test.\n\n### Plan for test failure\n\nWe regret to inform you that most of the quality time you spend with your tests will be when they are inexplicably failing.\n\n> In its purest form, automating testing consists of three activities: writing tests, running tests, and **reacting to test failures**....\n>\n> Remember that tests are often revisited only when something breaks.\n> When you are called to fix a broken test that you have never seen before, you will be thankful someone took the time to make it easy to understand.\n> Code is read far more than it is written, so make sure you write the test you'd like to read!\n>\n> From the book Software Engineering at Google, [Chapter 11](https://abseil.io/resources/swe-book/html/ch11.html)\n\nMost of us don't work on a code base the size of Google.\nBut even in a team of one, tests that you wrote six months ago might as well have been written by someone else.\nEspecially when they are failing.\n\nWhen we do reverse dependency checks, often involving hundreds or thousands of CRAN packages, we have to inspect test failures to determine if changes in our packages are to blame.\nAs a result, we regularly engage with failing tests in other people's packages, which leaves us with lots of opinions about practices that create unnecessary testing pain.\n\nTest troubleshooting nirvana looks like this: In a fresh R session, you can do `devtools::load_all()` and immediately run an individual test or walk through it line-by-line.\nThere is no need to hunt around for setup code that has to be run manually first, that is found elsewhere in the test file or perhaps in a different file altogether.\nTest-related code that lives in an unconventional location causes extra self-inflicted pain when you least need it.\n\nConsider this extreme and abstract example of a test that is difficult to troubleshoot due to implicit dependencies on free-range code:\n\n```{r, eval = FALSE}\n# dozens or hundreds of lines of top-level code, interspersed with other tests,\n# which you must read and selectively execute\n\ntest_that(\"f() works\", {\n  x <- function_from_some_dependency(object_with_unknown_origin)\n  expect_equal(f(x), 2.5)\n})\n```\n\nThis test is much easier to drop in on if dependencies are invoked in the normal way, i.e. via `::`, and test objects are created inline:\n\n```{r, eval = FALSE}\n# dozens or hundreds of lines of self-sufficient, self-contained tests,\n# all of which you can safely ignore!\n\ntest_that(\"f() works\", {\n  useful_thing <- ...\n  x <- somePkg::someFunction(useful_thing)\n  expect_equal(f(x), 2.5)\n})\n```\n\nThis test is self-sufficient.\nThe code inside `{ ... }` explicitly creates any necessary objects or conditions and makes explicit calls to any helper functions.\nThis test doesn't rely on objects or dependencies that happen to be ambiently available.\n\nSelf-sufficient, self-contained tests are a win-win: It is literally safer to design tests this way and it also makes tests much easier for humans to troubleshoot later.\n\n### Repetition is OK\n\nOne obvious consequence of our suggestion to minimize code with \"file scope\" is that your tests will probably have some repetition.\nAnd that's OK!\nWe're going to make the controversial recommendation that you tolerate a fair amount of duplication in test code, i.e. you can relax some of your DRY (\"don't repeat yourself\") tendencies.\n\n> Keep the reader in your test function.\n> Good production code is well-factored; good test code is obvious.\n> ... think about what will make the problem obvious when a test fails.\n>\n> From the blog post [Why Good Developers Write Bad Unit Tests](https://mtlynch.io/good-developers-bad-tests/)\n\nHere's a toy example to make things concrete.\n\n```{r}\ntest_that(\"multiplication works\", {\n  useful_thing <- 3\n  expect_equal(2 * useful_thing, 6)\n})\n\ntest_that(\"subtraction works\", {\n  useful_thing <- 3\n  expect_equal(5 - useful_thing, 2)\n})\n```\n\nIn real life, `useful_thing` is usually a more complicated object that somehow feels burdensome to instantiate.\nNotice how `useful_thing <- 3` appears in more than one place.\nConventional wisdom says we should DRY this code out.\nIt's tempting to just move `useful_thing`'s definition outside of the tests:\n\n```{r}\nuseful_thing <- 3\n\ntest_that(\"multiplication works\", {\n  expect_equal(2 * useful_thing, 6)\n})\n\ntest_that(\"subtraction works\", {\n  expect_equal(5 - useful_thing, 2)\n})\n```\n\nBut we really do think the first form, with the repetition, is often the better choice.\n\nAt this point, many readers might be thinking \"but the code I might have to repeat is much longer than 1 line!\".\nBelow we describe the use of test fixtures.\nThis can often reduce complicated situations back to something that resembles this simple example.\n\n### Remove tension between interactive and automated testing {#sec-testing-design-tension}\n\nYour test code will be executed in two different settings:\n\n-   Interactive test development and maintenance, which includes tasks like:\n    -   Initial test creation\n    -   Modifying tests to adapt to change\n    -   Debugging test failure\n-   Automated test runs, which is accomplished with functions such as:\n    -   Single file: `devtools::test_active_file()`, `testthat::test_file()`\n    -   Whole package: `devtools::test()`, `devtools::check()`\n\nAutomated testing of your whole package is what takes priority.\nThis is ultimately the whole point of your tests.\nHowever, the interactive experience is clearly important for the humans doing this work.\nTherefore it's important to find a pleasant workflow, but also to ensure that you don't rig anything for interactive convenience that actually compromises the health of the test suite.\n\nThese two modes of test-running should not be in conflict with each other.\nIf you perceive tension between these two modes, this can indicate that you're not taking full advantage of some of testthat's features and the way it's designed to work with `devtools::load_all()`.\n\nWhen working on your tests, use `load_all()`, just like you do when working below `R/`.\nBy default, `load_all()` does all of these things:\n\n-   Simulates re-building, re-installing, and re-loading your package.\n-   Makes everything in your package's namespace available, including unexported functions and objects and anything you've imported from another package.\n-   Attaches testthat, i.e. does `library(testthat)`.\n-   Runs test helper files, i.e. executes `test/testthat/helper.R` (more on that below).\n\nThis eliminates the need for any `library()` calls below `tests/testthat/`, for the vast majority of R packages.\nAny instance of `library(testthat)` is clearly no longer necessary.\nLikewise, any instance of attaching one of your dependencies via `library(somePkg)` is unnecessary.\nIn your tests, if you need to call functions from somePkg, do it just as you do below `R/`.\nIf you have imported the function into your namespace, use `fun()`.\nIf you have not, use `somePkg::fun()`.\nIt's fair to say that `library(somePkg)` in the tests should be about as rare as taking a dependency via `Depends`, i.e. there is almost always a better alternative.\n\nUnnecessary calls to `library(somePkg)` in test files have a real downside, because they actually change the R landscape.\n`library()` alters the search path.\nThis means the circumstances under which you are testing may not necessarily reflect the circumstances under which your package will be used.\nThis makes it easier to create subtle test bugs, which you will have to unravel in the future.\n\nOne other function that should almost never appear below `tests/testhat/` is `source()`.\nThere are several special files with an official role in testthat workflows (see below), not to mention the entire R package machinery, that provide better ways to make functions, objects, and other logic available in your tests.\n\n## Files relevant to testing {#sec-tests-files-overview}\n\nHere we review which package files are especially relevant to testing and, more generally, best practices for interacting with the file system from your tests.\n\n### Hiding in plain sight: files below `R/`\n\nThe most important functions you'll need to access from your tests are clearly those in your package!\nHere we're talking about everything that's defined below `R/`.\nThe functions and other objects defined by your package are always available when testing, regardless of whether they are exported or not.\nFor interactive work, `devtools::load_all()` takes care of this.\nDuring automated testing, this is taken care of internally by testthat.\n\nThis implies that test helpers can absolutely be defined below `R/` and used freely in your tests.\nIt might make sense to gather such helpers in a clearly marked file, such as one of these:\n\n```         \n.                              \n├── ...\n└── R\n    ├── ...\n    ├── test-helpers.R\n    ├── test-utils.R\n    ├── testthat.R\n    ├── utils-testing.R\n    └── ...\n```\n\nFor example, the dbplyr package uses [`R/testthat.R`](https://github.com/tidyverse/dbplyr/blob/e8bfa760a465cd7d8fa45cc53d4435ee1fbd2361/R/testthat.R) to define a couple of helpers to facilitate comparisons and expectations involving `tbl` objects, which is used to represent database tables.\n\n```{r}\n#| eval: false\ncompare_tbl <- function(x, y, label = NULL, expected.label = NULL) {\n  testthat::expect_equal(\n    arrange(collect(x), dplyr::across(everything())),\n    arrange(collect(y), dplyr::across(everything())),\n    label = label,\n    expected.label = expected.label\n  )\n}\n\nexpect_equal_tbls <- function(results, ref = NULL, ...) {\n  # code that gets things ready ...\n\n  for (i in seq_along(rest)) {\n    compare_tbl(\n      rest[[i]], ref,\n      label = names(rest)[[i]],\n      expected.label = ref_name\n    )\n  }\n\n  invisible(TRUE)\n}\n```\n\n### `tests/testthat.R`\n\nRecall the initial testthat setup described in @sec-tests-mechanics-workflow: The standard `tests/testthat.R` file looks like this:\n\n```{r eval = FALSE}\nlibrary(testthat)\nlibrary(pkg)\n\ntest_check(\"pkg\")\n```\n\nWe repeat the advice to not edit `tests/testthat.R`.\nIt is run during `R CMD check` (and, therefore, `devtools::check()`), but is not used in most other test-running scenarios (such as `devtools::test()` or `devtools::test_active_file()` or during interactive development).\nDo not attach your dependencies here with `library()`.\nCall them in your tests in the same manner as you do below `R/` (@sec-dependencies-in-imports-in-tests, @sec-dependencies-in-suggests-in-tests).\n\n### Testthat helper files\n\nAnother type of file that is always executed by `load_all()` and at the beginning of automated testing is a helper file, defined as any file below `tests/testthat/` that begins with `helper`.\nHelper files are a mighty weapon in the battle to eliminate code floating around at the top-level of test files.\nHelper files are a prime example of what we mean when we recommend moving such code into a broader scope.\nObjects or functions defined in a helper file are available to all of your tests.\n\nIf you have just one such file, you should probably name it `helper.R`.\nIf you organize your helpers into multiple files, you could include a suffix with additional info.\nHere are examples of how such files might look:\n\n```         \n.                              \n├── ...\n└── tests\n    ├── testthat\n    │   ├── helper.R\n    │   ├── helper-blah.R\n    │   ├── helper-foo.R    \n    │   ├── test-foofy.R\n    │   └── (more test files)\n    └── testthat.R\n```\n\nMany developers use helper files to define custom test helper functions, which we describe in detail in @sec-testing-advanced.\nCompared to defining helpers below `R/`, some people find that `tests/testthat/helper.R` makes it more clear that these utilities are specifically for testing the package.\nThis location also feels more natural if your helpers rely on testthat functions.\nFor example, [usethis](https://github.com/r-lib/usethis/blob/main/tests/testthat/helper.R) and [vroom](https://github.com/tidyverse/vroom/blob/main/tests/testthat/helper.R) both have fairly extensive `tests/testthat/helper.R` files that define many custom test helpers.\nHere are two very simple usethis helpers that check that the currently active project (usually an ephemeral test project) has a specific file or folder:\n\n```{r}\nexpect_proj_file <- function(...) expect_true(file_exists(proj_path(...)))\nexpect_proj_dir <- function(...) expect_true(dir_exists(proj_path(...)))\n```\n\nA helper file is also a good location for setup code that is needed for its side effects.\nThis is a case where `tests/testthat/helper.R` is clearly more appropriate than a file below `R/`.\nFor example, in an API-wrapping package, `helper.R` is a good place to (attempt to) authenticate with the testing credentials[^testing-design-2].\n\n[^testing-design-2]: googledrive does this in <https://github.com/tidyverse/googledrive/blob/906680f84b2cec2e4553978c9711be8d42ba33f7/tests/testthat/helper.R#L1-L10>.\n\n### Testthat setup files\n\nTestthat has one more special file type: setup files, defined as any file below `test/testthat/` that begins with `setup`.\nHere's an example of how that might look:\n\n```         \n.                              \n├── ...\n└── tests\n    ├── testthat\n    │   ├── helper.R\n    │   ├── setup.R\n    │   ├── test-foofy.R\n    │   └── (more test files)\n    └── testthat.R\n```\n\nA setup file is handled almost exactly like a helper file, but with two big differences:\n\n-   Setup files are not executed by `devtools::load_all()`.\n-   Setup files often contain the corresponding teardown code.\n\nSetup files are good for global test setup that is tailored for test execution in non-interactive or remote environments.\nFor example, you might turn off behaviour that's aimed at an interactive user, such as messaging or writing to the clipboard.\n\nIf any of your setup should be reversed after test execution, you should also include the necessary teardown code in `setup.R`[^testing-design-3].\nWe recommend maintaining teardown code alongside the setup code, in `setup.R`, because this makes it easier to ensure they stay in sync.\nThe artificial environment `teardown_env()` exists as a magical handle to use in `withr::defer()` and `withr::local_*()` / `withr::with_*()`.\n\n[^testing-design-3]: A legacy approach (which still works, but is no longer recommended) is to put teardown code in `tests/testthat/teardown.R`.\n\nHere's a `setup.R` example from the reprex package, where we turn off clipboard and HTML preview functionality during testing:\n\n```{r eval = FALSE}\nop <- options(reprex.clipboard = FALSE, reprex.html_preview = FALSE)\n\nwithr::defer(options(op), teardown_env())\n```\n\nSince we are just modifying options here, we can be even more concise and use the pre-built function `withr::local_options()` and pass `teardown_env()` as the `.local_envir`:\n\n```{r eval = FALSE}\nwithr::local_options(\n  list(reprex.clipboard = FALSE, reprex.html_preview = FALSE),\n  .local_envir = teardown_env()\n)\n```\n\n### Files ignored by testthat\n\ntestthat only automatically executes files where these are both true:\n\n-   File is a direct child of `tests/testthat/`\n-   File name starts with one of the specific strings:\n    -   `helper`\n    -   `setup`\n    -   `test`\n\nIt is fine to have other files or directories in `tests/testthat/`, but testthat won't automatically do anything with them (other than the `_snaps` directory, which holds snapshots).\n\n### Storing test data\n\nMany packages contain files that hold test data.\nWhere should these be stored?\nThe best location is somewhere below `tests/testthat/`, often in a subdirectory, to keep things neat.\nBelow is an example, where `useful_thing1.rds` and `useful_thing2.rds` hold objects used in the test files.\n\n```         \n.\n├── ...\n└── tests\n    ├── testthat\n    │   ├── fixtures\n    │   │   ├── make-useful-things.R\n    │   │   ├── useful_thing1.rds\n    │   │   └── useful_thing2.rds\n    │   ├── helper.R\n    │   ├── setup.R\n    │   └── (all the test files)\n    └── testthat.R\n```\n\nThen, in your tests, use `testthat::test_path()` to build a robust filepath to such files.\n\n```{r eval = FALSE}\ntest_that(\"foofy() does this\", {\n  useful_thing <- readRDS(test_path(\"fixtures\", \"useful_thing1.rds\"))\n  # ...\n})\n```\n\n`testthat::test_path()` is extremely handy, because it produces the correct path in the two important modes of test execution:\n\n-   Interactive test development and maintenance, where working directory is presumably set to the top-level of the package.\n-   Automated testing, where working directory is usually set to something below `tests/`.\n\n### Where to write files during testing {#sec-tests-files-where-write}\n\nIf it's easy to avoid writing files from your tests, that is definitely the best plan.\nBut there are many times when you really must write files.\n\n**You should only write files inside the session temp directory.** Do not write into your package's `tests/` directory.\nDo not write into the current working directory.\nDo not write into the user's home directory.\nEven though you are writing into the session temp directory, you should still clean up after yourself, i.e. delete any files you've written.\n\nMost package developers don't want to hear this, because it sounds like a hassle.\nBut it's not that burdensome once you get familiar with a few techniques and build some new habits.\nA high level of file system discipline also eliminates various testing bugs and will absolutely make your CRAN life run more smoothly.\n\nThis test is from roxygen2 and demonstrates everything we recommend:\n\n```{r eval = FALSE}\ntest_that(\"can read from file name with utf-8 path\", {\n  path <- withr::local_tempfile(\n    pattern = \"Universit\\u00e0-\",\n    lines = c(\"#' @include foo.R\", NULL)\n  )\n  expect_equal(find_includes(path), \"foo.R\")\n})\n```\n\n`withr::local_tempfile()` creates a file within the session temp directory whose lifetime is tied to the \"local\" environment -- in this case, the execution environment of an individual test.\nIt is a wrapper around `base::tempfile()` and passes, e.g., the `pattern` argument through, so you have some control over the file name.\nYou can optionally provide `lines` to populate the file with at creation time or you can write to the file in all the usual ways in subsequent steps.\nFinally, with no special effort on your part, the temporary file will automatically be deleted at the end of the test.\n\nSometimes you need even more control over the file name.\nIn that case, you can use `withr::local_tempdir()` to create a self-deleting temporary directory and write intentionally-named files inside this directory.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["plausible.html"],"output-file":"testing-design.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","bibliography":["book.bib"],"editor":"source","mainfont":"Microsoft Yahei","theme":["cosmo","custom.scss"]},"extensions":{"book":{"multiFile":true}}}}}