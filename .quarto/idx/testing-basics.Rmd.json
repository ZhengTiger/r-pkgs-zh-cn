{"title":"Testing basics","markdown":{"headingText":"Testing basics","headingAttr":{"id":"sec-testing-basics","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n```{r, echo = FALSE}\nsource(\"common.R\")\n```\n\nTesting is a vital part of package development: it ensures that your code does what you want.\nTesting, however, adds an additional step to your workflow.\nTo make this task easier and more effective this chapter will show you how to do formal automated testing using the testthat package.\n\nThe first stage of your testing journey is to become convinced that testing has enough benefits to justify the work.\nFor some of us, this is easy to accept.\nOthers must learn the hard way.\n\nOnce you've decided to embrace automated testing, it's time to learn some mechanics and figure out where testing fits into your development workflow.\n\nAs you and your R packages evolve, you'll start to encounter testing situations where it's fruitful to use techniques that are somewhat specific to testing and differ from what we do below `R/`.\n\n## Why is formal testing worth the trouble?\n\nUp until now, your workflow probably looks like this:\n\n1.  Write a function.\n2.  Load it with `devtools::load_all()`, maybe via Ctrl/Cmd + Shift + L.\n3.  Experiment with it in the console to see if it works.\n4.  Rinse and repeat.\n\nWhile you *are* testing your code in this workflow, you're only doing it informally.\nThe problem with this approach is that when you come back to this code in 3 months time to add a new feature, you've probably forgotten some of the informal tests you ran the first time around.\nThis makes it very easy to break code that used to work.\n\nMany of us embrace automated testing when we realize we're re-fixing a bug for the second or fifth time.\nWhile writing code or fixing bugs, we might perform some interactive tests to make sure the code we're working on does what we want.\nBut it's easy to forget all the different use cases you need to check, if you don't have a system for storing and re-running the tests.\nThis is a common practice among R programmers.\nThe problem is not that you don't test your code, it's that you don't automate your tests.\n\nIn this chapter you'll learn how to transition from informal *ad hoc* testing, done interactively in the console, to automated testing (also known as unit testing).\nWhile turning casual interactive tests into formal tests requires a little more work up front, it pays off in four ways:\n\n-   Fewer bugs.\n    Because you're explicit about how your code should behave, you will have fewer bugs.\n    The reason why is a bit like the reason double entry book-keeping works: because you describe the behaviour of your code in two places, both in your code and in your tests, you are able to check one against the other.\n\n    With informal testing, it's tempting to just explore typical and authentic usage, similar to writing examples.\n    However, when writing formal tests, it's natural to adopt a more adversarial mindset and to anticipate how unexpected inputs could break your code.\n\n    If you always introduce new tests when you add a new feature or function, you'll prevent many bugs from being created in the first place, because you will proactively address pesky edge cases.\n    Tests also keep you from (re-)breaking one feature, when you're tinkering with another.\n\n-   Better code structure.\n    Code that is well designed tends to be easy to test and you can turn this to your advantage.\n    If you are struggling to write tests, consider if the problem is actually the design of your function(s).\n    The process of writing tests is a great way to get free, private, and personalized feedback on how well-factored your code is.\n    If you integrate testing into your development workflow (versus planning to slap tests on \"later\"), you'll subject yourself to constant pressure to break complicated operations into separate functions that work in isolation.\n    Functions that are easier to test are usually easier to understand and re-combine in new ways.\n\n-   Call to action.\n    When we start to fix a bug, we first like to convert it into a (failing) test.\n    This is wonderfully effective at making your goal very concrete: make this test pass.\n    This is basically a special case of a general methodology known as test driven development.\n\n-   Robust code.\n    If you know that all the major functionality of your package is well covered by the tests, you can confidently make big changes without worrying about accidentally breaking something.\n    This provides a great reality check when you think you've discovered some brilliant new way to simplify your package.\n    Sometimes such \"simplifications\" fail to account for some important use case and your tests will save you from yourself.\n\n## Introducing testthat\n\nThis chapter describes how to test your R package using the testthat package: <https://testthat.r-lib.org>\n\nIf you're familiar with frameworks for unit testing in other languages, you should note that there are some fundamental differences with testthat.\nThis is because R is, at heart, more a functional programming language than an object-oriented programming language.\nFor instance, because R's main object-oriented systems (S3 and S4) are based on generic functions (i.e., a method implements a generic function for a specific class), testing approaches built around objects and methods don't make much sense.\n\ntestthat 3.0.0 (released 2020-10-31) introduced the idea of an **edition** of testthat, specifically the third edition of testthat, which we refer to as testthat 3e.\nAn edition is a bundle of behaviors that you have to explicitly choose to use, allowing us to make otherwise backward incompatible changes.\nThis is particularly important for testthat since it has a very large number of packages that use it (almost 5,000 at last count).\nTo use testthat 3e, you must have a version of testthat \\>= 3.0.0 **and** explicitly opt-in to the third edition behaviors.\nThis allows testthat to continue to evolve and improve without breaking historical packages that are in a rather passive maintenance phase.\nYou can learn more in the [testthat 3e article](https://testthat.r-lib.org/articles/third-edition.html) and the blog post [Upgrading to testthat edition 3](https://www.tidyverse.org/blog/2022/02/upkeep-testthat-3/).\n\nWe recommend testthat 3e for all new packages and we recommend updating existing, actively maintained packages to use testthat 3e.\nUnless we say otherwise, this chapter describes testthat 3e.\n\n## Test mechanics and workflow {#sec-tests-mechanics-workflow}\n\n### Initial setup\n\nTo setup your package to use testthat, run:\n\n```{r, eval = FALSE}\nusethis::use_testthat(3)\n```\n\nThis will:\n\n1.  Create a `tests/testthat/` directory.\n\n2.  Add testthat to the `Suggests` field in the `DESCRIPTION` and specify testthat 3e in the `Config/testthat/edition` field.\n    The affected `DESCRIPTION` fields might look like:\n\n    ```         \n    Suggests: testthat (>= 3.0.0)\n    Config/testthat/edition: 3\n    ```\n\n3.  Create a file `tests/testthat.R` that runs all your tests when `R CMD check` runs (@sec-workflow101-r-cmd-check).\n    For a package named \"pkg\", the contents of this file will be something like:\n\n    ```{r}\n    #| eval: false\n    library(testthat)\n    library(pkg)\n\n    test_check(\"pkg\")\n    ```\n\nThis initial setup is usually something you do once per package.\nHowever, even in a package that already uses testthat, it is safe to run `use_testthat(3)`, when you're ready to opt-in to testthat 3e.\n\nDo not edit `tests/testthat.R`!\nIt is run during `R CMD check` (and, therefore, `devtools::check()`), but is not used in most other test-running scenarios (such as `devtools::test()` or `devtools::test_active_file()`).\nIf you want to do something that affects all of your tests, there is almost always a better way than modifying the boilerplate `tests/testthat.R` script.\nThis chapter details many different ways to make objects and logic available during testing.\n\n### Create a test\n\nAs you define functions in your package, in the files below `R/`, you add the corresponding tests to `.R` files in `tests/testthat/`.\nWe strongly recommend that the organisation of test files match the organisation of `R/` files, discussed in @sec-code-organising: The `foofy()` function (and its friends and helpers) should be defined in `R/foofy.R` and their tests should live in `tests/testthat/test-foofy.R`.\n\n```         \nR                                     tests/testthat\n└── foofy.R                           └── test-foofy.R\n    foofy <- function(...) {...}          test_that(\"foofy does this\", {...})\n                                          test_that(\"foofy does that\", {...})\n```\n\nEven if you have different conventions for file organisation and naming, note that testthat tests **must** live in files below `tests/testthat/` and these file names **must** begin with `test`.\nThe test file name is displayed in testthat output, which provides helpful context[^testing-basics-1].\n\n[^testing-basics-1]: The legacy function `testthat::context()` is superseded now and its use in new or actively maintained code is discouraged.\n    In testthat 3e, `context()` is formally deprecated; you should just remove it.\n    Once you adopt an intentional, synchronized approach to the organisation of files below `R/` and `tests/testthat/`, the necessary contextual information is right there in the file name, rendering the legacy `context()` superfluous.\n\n<!-- Hadley thinks this is too much detail about use_r()/use_test(). I will likely agree when I revisit this later. Leaving it for now. -->\n\nusethis offers a helpful pair of functions for creating or toggling between files:\n\n-   `usethis::use_r()`\n-   `usethis::use_test()`\n\nEither one can be called with a file (base) name, in order to create a file *de novo* and open it for editing:\n\n```{r, eval = FALSE}\nuse_r(\"foofy\")    # creates and opens R/foofy.R\nuse_test(\"blarg\") # creates and opens tests/testthat/test-blarg.R\n```\n\nThe `use_r()` / `use_test()` duo has some convenience features that make them \"just work\" in many common situations:\n\n-   When determining the target file, they can deal with the presence or absence of the `.R` extension and the `test-` prefix.\n    -   Equivalent: `use_r(\"foofy.R\")`, `use_r(\"foofy\")`\n    -   Equivalent: `use_test(\"test-blarg.R\")`, `use_test(\"blarg.R\")`, `use_test(\"blarg\")`\n-   If the target file already exists, it is opened for editing. Otherwise, the target is created and then opened for editing.\n\n::: callout-tip\n## RStudio\n\nIf `R/foofy.R` is the active file in your source editor, you can even call `use_test()` with no arguments!\nThe target test file can be inferred: if you're editing `R/foofy.R`, you probably want to work on the companion test file, `tests/testthat/test-foofy.R`.\nIf it doesn't exist yet, it is created and, either way, the test file is opened for editing.\nThis all works the other way around also.\nIf you're editing `tests/testthat/test-foofy.R`, a call to `use_r()` (optionally, creates and) opens `R/foofy.R`.\n:::\n\nBottom line: `use_r()` / `use_test()` are handy for initially creating these file pairs and, later, for shifting your attention from one to the other.\n\nWhen `use_test()` creates a new test file, it inserts an example test:\n\n```{r, eval = FALSE}\ntest_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n```\n\nYou will replace this with your own description and logic, but it's a nice reminder of the basic form:\n\n-   A test file holds one or more `test_that()` tests.\n-   Each test describes what it's testing: e.g. \"multiplication works\".\n-   Each test has one or more expectations: e.g. `expect_equal(2 * 2, 4)`.\n\nBelow we go into much more detail about how to test your own functions.\n\n### Run tests\n\nDepending on where you are in the development cycle, you'll run your tests at various scales.\nWhen you are rapidly iterating on a function, you might work at the level of individual tests.\nAs the code settles down, you'll run entire test files and eventually the entire test suite.\n\n**Micro-iteration**: This is the interactive phase where you initiate and refine a function and its tests in tandem.\nHere you will run `devtools::load_all()` often, and then execute individual expectations or whole tests interactively in the console.\nNote that `load_all()` attaches testthat, so it puts you in the perfect position to test drive your functions and to execute individual tests and expectations.\n\n```{r, eval = FALSE}\n# tweak the foofy() function and re-load it\ndevtools::load_all()\n\n# interactively explore and refine expectations and tests\nexpect_equal(foofy(...), EXPECTED_FOOFY_OUTPUT)\n\ntest_that(\"foofy does good things\", {...})\n```\n\n**Mezzo-iteration**: As one file's-worth of functions and their associated tests start to shape up, you will want to execute the entire file of associated tests, perhaps with `testthat::test_file()`:\n\n```{=html}\n<!-- `devtools::test_file()` exists, but is deprecated, because of the collision.\n\nConsider marking as defunct / removing before the book is published. -->\n```\n```{r, eval = FALSE}\ntestthat::test_file(\"tests/testthat/test-foofy.R\")\n```\n\n::: callout-tip\n## RStudio\n\nIn RStudio, you have a couple shortcuts for running a single test file.\n\nIf the target test file is the active file, you can use the \"Run Tests\" button in the upper right corner of the source editor.\n\nThere is also a useful function, `devtools::test_active_file()`.\nIt infers the target test file from the active file and, similar to how `use_r()` and `use_test()` work, it works regardless of whether the active file is a test file or a companion `R/*.R` file.\nYou can invoke this via \"Run a test file\" in the Addins menu.\nHowever, for heavy users (like us!), we recommend [binding this to a keyboard shortcut](https://support.rstudio.com/hc/en-us/articles/206382178-Customizing-Keyboard-Shortcuts-in-the-RStudio-IDE); we use Ctrl/Cmd + T.\n:::\n\n**Macro-iteration**: As you near the completion of a new feature or bug fix, you will want to run the entire test suite.\n\nMost frequently, you'll do this with `devtools::test()`:\n\n```{r, eval = FALSE}\ndevtools::test()\n```\n\nThen eventually, as part of `R CMD check` with `devtools::check()`:\n\n```{r, eval = FALSE}\ndevtools::check()\n```\n\n::: callout-tip\n## RStudio\n\n`devtools::test()` is mapped to Ctrl/Cmd + Shift + T.\n`devtools::check()` is mapped to Ctrl/Cmd + Shift + E.\n:::\n\n```{=html}\n<!-- We'll probably want to replace this example eventually, but it's a decent placeholder.\nThe test failure is something highly artificial I created very quickly. \nIt would be better to use an example that actually makes sense, if someone elects to really read and think about it.-->\n```\nThe output of `devtools::test()` looks like this:\n\n```         \ndevtools::test()\nℹ Loading usethis\nℹ Testing usethis\n✓ | F W S  OK | Context\n✓ |         1 | addin [0.1s]\n✓ |         6 | badge [0.5s]\n   ...\n✓ |        27 | github-actions [4.9s]\n   ...\n✓ |        44 | write [0.6s]\n\n══ Results ═════════════════════════════════════════════════════════════════\nDuration: 31.3 s\n\n── Skipped tests  ──────────────────────────────────────────────────────────\n• Not on GitHub Actions, Travis, or Appveyor (3)\n\n[ FAIL 1 | WARN 0 | SKIP 3 | PASS 728 ]\n```\n\nTest failure is reported like this:\n\n```         \nFailure (test-release.R:108:3): get_release_data() works if no file found\nres$Version (`actual`) not equal to \"0.0.0.9000\" (`expected`).\n\n`actual`:   \"0.0.0.1234\"\n`expected`: \"0.0.0.9000\"\n```\n\nEach failure gives a description of the test (e.g., \"get_release_data() works if no file found\"), its location (e.g., \"test-release.R:108:3\"), and the reason for the failure (e.g., \"res\\$Version (`actual`) not equal to\"0.0.0.9000\" (`expected`)\").\n\nThe idea is that you'll modify your code (either the functions defined below `R/` or the tests in `tests/testthat/`) until all tests are passing.\n\n## Test organisation\n\nA test file lives in `tests/testthat/`.\nIts name must start with `test`.\nWe will inspect and execute a test file from the stringr package.\n\n<!-- https://github.com/hadley/r-pkgs/issues/778 -->\n\nBut first, for the purposes of rendering this book, we must attach stringr and testthat.\nNote that in real-life test-running situations, this is taken care of by your package development tooling:\n\n-   During interactive development, `devtools::load_all()` makes testthat and the package-under-development available (both its exported and unexported functions).\n-   During arms-length test execution, this is taken care of by `devtools::test_active_file()`, `devtools::test()`, and `tests/testthat.R`.\n\n::: callout-important\nYour test files should not include these `library()` calls.\nWe also explicitly request testthat edition 3, but in a real package this will be declared in DESCRIPTION.\n\n```{r}\nlibrary(testthat)\nlibrary(stringr)\nlocal_edition(3)\n```\n:::\n\nHere are the contents of `tests/testthat/test-dup.r` from stringr:\n\n```{r}\ntest_that(\"basic duplication works\", {\n  expect_equal(str_dup(\"a\", 3), \"aaa\")\n  expect_equal(str_dup(\"abc\", 2), \"abcabc\")\n  expect_equal(str_dup(c(\"a\", \"b\"), 2), c(\"aa\", \"bb\"))\n  expect_equal(str_dup(c(\"a\", \"b\"), c(2, 3)), c(\"aa\", \"bbb\"))\n})\n\ntest_that(\"0 duplicates equals empty string\", {\n  expect_equal(str_dup(\"a\", 0), \"\")\n  expect_equal(str_dup(c(\"a\", \"b\"), 0), rep(\"\", 2))\n})\n\ntest_that(\"uses tidyverse recycling rules\", {\n  expect_error(str_dup(1:2, 1:3), class = \"vctrs_error_incompatible_size\")\n})\n```\n\nThis file shows a typical mix of tests:\n\n-   \"basic duplication works\" tests typical usage of `str_dup()`.\n-   \"0 duplicates equals empty string\" probes a specific edge case.\n-   \"uses tidyverse recycling rules\" checks that malformed input results in a specific kind of error.\n\nTests are organised hierarchically: **expectations** are grouped into **tests** which are organised in **files**:\n\n-   A **file** holds multiple related tests.\n    In this example, the file `tests/testthat/test-dup.r` has all of the tests for the code in `R/dup.r`.\n\n-   A **test** groups together multiple expectations to test the output from a simple function, a range of possibilities for a single parameter from a more complicated function, or tightly related functionality from across multiple functions.\n    This is why they are sometimes called **unit** tests.\n    Each test should cover a single unit of functionality.\n    A test is created with `test_that(desc, code)`.\n\n    It's common to write the description (`desc`) to create something that reads naturally, e.g. `test_that(\"basic duplication works\", { ... })`.\n    A test failure report includes this description, which is why you want a concise statement of the test's purpose, e.g. a specific behaviour.\n\n-   An **expectation** is the atom of testing.\n    It describes the expected result of a computation: Does it have the right value and right class?\n    Does it produce an error when it should?\n    An expectation automates visual checking of results in the console.\n    Expectations are functions that start with `expect_`.\n\nYou want to arrange things such that, when a test fails, you'll know what's wrong and where in your code to look for the problem.\nThis motivates all our recommendations regarding file organisation, file naming, and the test description.\nFinally, try to avoid putting too many expectations in one test - it's better to have more smaller tests than fewer larger tests.\n\n## Expectations\n\nAn expectation is the finest level of testing.\nIt makes a binary assertion about whether or not an object has the properties you expect.\nThis object is usually the return value from a function in your package.\n\nAll expectations have a similar structure:\n\n-   They start with `expect_`.\n\n-   They have two main arguments: the first is the actual result, the second is what you expect.\n\n-   If the actual and expected results don't agree, testthat throws an error.\n\n-   Some expectations have additional arguments that control the finer points of comparing an actual and expected result.\n\nWhile you'll normally put expectations inside tests inside files, you can also run them directly.\nThis makes it easy to explore expectations interactively.\nThere are more than 40 expectations in the testthat package, which can be explored in testthat's [reference index](https://testthat.r-lib.org/reference/index.html).\nWe're only going to cover the most important expectations here.\n\n### Testing for equality\n\n`expect_equal()` checks for equality, with some reasonable amount of numeric tolerance:\n\n```{r, error = TRUE}\nexpect_equal(10, 10)\nexpect_equal(10, 10L)\nexpect_equal(10, 10 + 1e-7)\nexpect_equal(10, 11)\n```\n\nIf you want to test for exact equivalence, use `expect_identical()`.\n\n```{r, error = TRUE}\nexpect_equal(10, 10 + 1e-7)\nexpect_identical(10, 10 + 1e-7)\n\nexpect_equal(2, 2L)\nexpect_identical(2, 2L)\n```\n\n### Testing errors\n\nUse `expect_error()` to check whether an expression throws an error.\nIt's the most important expectation in a trio that also includes `expect_warning()` and `expect_message()`.\nWe're going to emphasize errors here, but most of this also applies to warnings and messages.\n\nUsually you care about two things when testing an error:\n\n-   Does the code fail? Specifically, does it fail for the right reason?\n-   Does the accompanying message make sense to the human who needs to deal with the error?\n\nThe entry-level solution is to expect a specific type of condition:\n\n```{r, warning = TRUE, error = TRUE}\n1 / \"a\"\nexpect_error(1 / \"a\") \n\nlog(-1)\nexpect_warning(log(-1))\n```\n\nThis is a bit dangerous, though, especially when testing an error.\nThere are lots of ways for code to fail!\nConsider the following test:\n\n```{r}\nexpect_error(str_duq(1:2, 1:3))\n```\n\nThis expectation is intended to test the recycling behaviour of `str_dup()`.\nBut, due to a typo, it tests behaviour of a non-existent function, `str_duq()`.\nThe code throws an error and, therefore, the test above passes, but for the *wrong reason*.\nDue to the typo, the actual error thrown is about not being able to find the `str_duq()` function:\n\n```{r, error = TRUE}\nstr_duq(1:2, 1:3)\n```\n\nHistorically, the best defense against this was to assert that the condition message matches a certain regular expression, via the second argument, `regexp`.\n\n```{r}\nexpect_error(1 / \"a\", \"non-numeric argument\")\nexpect_warning(log(-1), \"NaNs produced\")\n```\n\nThis does, in fact, force our typo problem to the surface:\n\n```{r error = TRUE}\nexpect_error(str_duq(1:2, 1:3), \"recycle\")\n```\n\nRecent developments in both base R and rlang make it increasingly likely that conditions are signaled with a *class*, which provides a better basis for creating precise expectations.\nThat is exactly what you've already seen in this stringr example.\nThis is what the `class` argument is for:\n\n```{r, error = TRUE}\n# fails, error has wrong class\nexpect_error(str_duq(1:2, 1:3), class = \"vctrs_error_incompatible_size\")\n\n# passes, error has expected class\nexpect_error(str_dup(1:2, 1:3), class = \"vctrs_error_incompatible_size\")\n```\n\n```{=html}\n<!-- This advice feels somewhat at odds with Hadley's ambivalence about classed errors.\nI.e. I think he recommends using a classed condition only when there's a specific reason to.\nThen again, maybe the desire to test it is a legitimate reason? -->\n```\nIf you have the choice, express your expectation in terms of the condition's class, instead of its message.\nOften this is under your control, i.e. if your package signals the condition.\nIf the condition originates from base R or another package, proceed with caution.\nThis is often a good reminder to re-consider the wisdom of testing a condition that is not fully under your control in the first place.\n\nTo check for the *absence* of an error, warning, or message, use `expect_no_error()`:\n\n```{r}\nexpect_no_error(1 / 2)\n```\n\nOf course, this is functionally equivalent to simply executing `1 / 2` inside a test, but some developers find the explicit expectation expressive.\n\nIf you genuinely care about the condition's message, testthat 3e's snapshot tests are the best approach, which we describe next.\n\n### Snapshot tests {#sec-snapshot-tests}\n\nSometimes it's difficult or awkward to describe an expected result with code.\nSnapshot tests are a great solution to this problem and this is one of the main innovations in testthat 3e.\nThe basic idea is that you record the expected result in a separate, human-readable file.\nGoing forward, testthat alerts you when a newly computed result differs from the previously recorded snapshot.\nSnapshot tests are particularly suited to monitoring your package's user interface, such as its informational messages and errors.\nOther use cases include testing images or other complicated objects.\n\nWe'll illustrate snapshot tests using the waldo package.\nUnder the hood, testthat 3e uses waldo to do the heavy lifting of \"actual vs. expected\" comparisons, so it's good for you to know a bit about waldo anyway.\nOne of waldo's main design goals is to present differences in a clear and actionable manner, as opposed to a frustrating declaration that \"this differs from that and I know exactly how, but I won't tell you\".\nTherefore, the formatting of output from `waldo::compare()` is very intentional and is well-suited to a snapshot test.\nThe binary outcome of `TRUE` (actual == expected) vs. `FALSE` (actual != expected) is fairly easy to check and could get its own test.\nHere we're concerned with writing a test to ensure that differences are reported to the user in the intended way.\n\nwaldo uses a few different layouts for showing diffs, depending on various conditions.\nHere we deliberately constrain the width, in order to trigger a side-by-side layout.[^testing-basics-2]\n(We'll talk more about the withr package below.)\n\n[^testing-basics-2]: The actual waldo test that inspires this example targets an unexported helper function that produces the desired layout.\n    But this example uses an exported waldo function for simplicity.\n\n```{r}\nwithr::with_options(\n  list(width = 20),\n  waldo::compare(c(\"X\", letters), c(letters, \"X\"))\n)\n```\n\nThe two primary inputs differ at two locations: once at the start and once at the end.\nThis layout presents both of these, with some surrounding context, which helps the reader orient themselves.\n\nHere's how this would look as a snapshot test:\n\n```{=html}\n<!-- Actually using snapshot test technology here is hard.\nI can sort of see how it might be done, by looking at the source of testthat's vignette about snapshotting.\nFor the moment, I'm just faking it. -->\n```\n```{r eval = FALSE}\ntest_that(\"side-by-side diffs work\", {\n  withr::local_options(width = 20)\n  expect_snapshot(\n    waldo::compare(c(\"X\", letters), c(letters, \"X\"))\n  )\n})\n```\n\nIf you execute `expect_snapshot()` or a test containing `expect_snapshot()` interactively, you'll see this:\n\n```         \nCan't compare snapshot to reference when testing interactively\nℹ Run `devtools::test()` or `testthat::test_file()` to see changes\n```\n\nfollowed by a preview of the snapshot output.\n\nThis reminds you that snapshot tests only function when executed non-interactively, i.e. while running an entire test file or the entire test suite.\nThis applies both to recording snapshots and to checking them.\n\nThe first time this test is executed via `devtools::test()` or similar, you'll see something like this (assume the test is in `tests/testthat/test-diff.R`):\n\n```         \n── Warning (test-diff.R:63:3): side-by-side diffs work ─────────────────────\nAdding new snapshot:\nCode\n  waldo::compare(c(\n    \"X\", letters), c(\n    letters, \"X\"))\nOutput\n      old | new    \n  [1] \"X\" -        \n  [2] \"a\" | \"a\" [1]\n  [3] \"b\" | \"b\" [2]\n  [4] \"c\" | \"c\" [3]\n  \n       old | new     \n  [25] \"x\" | \"x\" [24]\n  [26] \"y\" | \"y\" [25]\n  [27] \"z\" | \"z\" [26]\n           - \"X\" [27]\n```\n\nThere is always a warning upon initial snapshot creation.\nThe snapshot is added to `tests/testthat/_snaps/diff.md`, under the heading \"side-by-side diffs work\", which comes from the test's description.\nThe snapshot looks exactly like what a user sees interactively in the console, which is the experience we want to check for.\nThe snapshot file is *also* very readable, which is pleasant for the package developer.\nThis readability extends to snapshot changes, i.e. when examining Git diffs and reviewing pull requests on GitHub, which helps you keep tabs on your user interface.\nGoing forward, as long as your package continues to re-capitulate the expected snapshot, this test will pass.\n\nIf you've written a lot of conventional unit tests, you can appreciate how well-suited snapshot tests are for this use case.\nIf we were forced to inline the expected output in the test file, there would be a great deal of quoting, escaping, and newline management.\nIronically, with conventional expectations, the output you expect your user to see tends to get obscured by a heavy layer of syntactical noise.\n\nWhat about when a snapshot test fails?\nLet's imagine a hypothetical internal change where the default labels switch from \"old\" and \"new\" to \"OLD\" and \"NEW\".\nHere's how this snapshot test would react:\n\n```         \n── Failure (test-diff.R:63:3): side-by-side diffs work──────────────────────────\nSnapshot of code has changed:\nold[3:15] vs new[3:15]\n  \"    \\\"X\\\", letters), c(\"\n  \"    letters, \\\"X\\\"))\"\n  \"Output\"\n- \"      old | new    \"\n+ \"      OLD | NEW    \"\n  \"  [1] \\\"X\\\" -        \"\n  \"  [2] \\\"a\\\" | \\\"a\\\" [1]\"\n  \"  [3] \\\"b\\\" | \\\"b\\\" [2]\"\n  \"  [4] \\\"c\\\" | \\\"c\\\" [3]\"\n  \"  \"\n- \"       old | new     \"\n+ \"       OLD | NEW     \"\nand 3 more ...\n\n* Run `snapshot_accept('diff')` to accept the change\n* Run `snapshot_review('diff')` to interactively review the change\n```\n\nThis diff is presented more effectively in most real-world usage, e.g. in the console, by a Git client, or via a Shiny app (see below).\nBut even this plain text version highlights the changes quite clearly.\nEach of the two loci of change is indicated with a pair of lines marked with `-` and `+`, showing how the snapshot has changed.\n\nYou can call `testthat::snapshot_review('diff')` to review changes locally in a Shiny app, which lets you skip or accept individual snapshots.\nOr, if all changes are intentional and expected, you can go straight to `testthat::snapshot_accept('diff')`.\nOnce you've re-synchronized your actual output and the snapshots on file, your tests will pass once again.\nIn real life, snapshot tests are a great way to stay informed about changes to your package's user interface, due to your own internal changes or due to changes in your dependencies or even R itself.\n\n`expect_snapshot()` has a few arguments worth knowing about:\n\n-   `cran = FALSE`: By default, snapshot tests are skipped if it looks like the tests are running on CRAN's servers.\n    This reflects the typical intent of snapshot tests, which is to proactively monitor user interface, but not to check for correctness, which presumably is the job of other unit tests which are not skipped.\n    In typical usage, a snapshot change is something the developer will want to know about, but it does not signal an actual defect.\n\n-   `error = FALSE`: By default, snapshot code is *not* allowed to throw an error.\n    See `expect_error()`, described above, for one approach to testing errors.\n    But sometimes you want to assess \"Does this error message make sense to a human?\" and having it laid out in context in a snapshot is a great way to see it with fresh eyes.\n    Specify `error = TRUE` in this case:\n\n    ```{r eval = FALSE}\n    expect_snapshot(error = TRUE,\n      str_dup(1:2, 1:3)\n    )\n    ```\n\n-   `transform`: Sometimes a snapshot contains volatile, insignificant elements, such as a temporary filepath or a timestamp.\n    The `transform` argument accepts a function, presumably written by you, to remove or replace such changeable text.\n    Another use of `transform` is to scrub sensitive information from the snapshot.\n\n-   `variant`: Sometimes snapshots reflect the ambient conditions, such as the operating system or the version of R or one of your dependencies, and you need a different snapshot for each variant.\n    This is an experimental and somewhat advanced feature, so if you can arrange things to use a single snapshot, you probably should.\n\nIn typical usage, testthat will take care of managing the snapshot files below `tests/testthat/_snaps/`.\nThis happens in the normal course of you running your tests and, perhaps, calling `testthat::snapshot_accept()`.\n\n### Shortcuts for other common patterns\n\nWe conclude this section with a few more expectations that come up frequently.\nBut remember that testthat has [many more pre-built expectations](https://testthat.r-lib.org/reference/index.html) than we can demonstrate here.\n\nSeveral expectations can be described as \"shortcuts\", i.e. they streamline a pattern that comes up often enough to deserve its own wrapper.\n\n-   `expect_match(object, regexp, ...)` is a shortcut that wraps `grepl(pattern = regexp, x = object, ...)`.\n    It matches a character vector input against a regular expression `regexp`.\n    The optional `all` argument controls whether all elements or just one element needs to match.\n    Read the `expect_match()` documentation to see how additional arguments, like `ignore.case = FALSE` or `fixed = TRUE`, can be passed down to `grepl()`.\n\n    ```{r, error = TRUE}\n    string <- \"Testing is fun!\"\n      \n    expect_match(string, \"Testing\") \n     \n    # Fails, match is case-sensitive\n    expect_match(string, \"testing\")\n      \n    # Passes because additional arguments are passed to grepl():\n    expect_match(string, \"testing\", ignore.case = TRUE)\n    ```\n\n-   `expect_length(object, n)` is a shortcut for `expect_equal(length(object), n)`.\n\n-   `expect_setequal(x, y)` tests that every element of `x` occurs in `y`, and that every element of `y` occurs in `x`.\n    But it won't fail if `x` and `y` happen to have their elements in a different order.\n\n-   `expect_s3_class()` and `expect_s4_class()` check that an object `inherit()`s from a specified class.\n    `expect_type()`checks the `typeof()` an object.\n\n    ```{r, error = TRUE}\n    model <- lm(mpg ~ wt, data = mtcars)\n    expect_s3_class(model, \"lm\")\n    expect_s3_class(model, \"glm\")\n    ```\n\n`expect_true()` and `expect_false()` are useful catchalls if none of the other expectations does what you need.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["plausible.html"],"output-file":"testing-basics.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","bibliography":["book.bib"],"editor":"visual","theme":["cosmo","custom.scss"]},"extensions":{"book":{"multiFile":true}}}}}